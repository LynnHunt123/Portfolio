{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1ee5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yfinance) (1.1.5)\n",
      "Requirement already satisfied: requests>=2.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yfinance) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yfinance) (1.19.5)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yfinance) (4.6.4)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Downloading multitasking-0.0.10.tar.gz (8.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.24.0->yfinance) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.24.0->yfinance) (2021.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.26->yfinance) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.26->yfinance) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.26->yfinance) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.26->yfinance) (2.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
      "Building wheels for collected packages: multitasking\n",
      "  Building wheel for multitasking (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multitasking: filename=multitasking-0.0.10-py3-none-any.whl size=8488 sha256=161ddbcac14c38e6f6ac753e22bd252f341b6044c3506eb7f259bab855714471\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/15/e6/fa/f4bf8d84e804547b3c1b1d4b09a671768502b32ca33ec60651\n",
      "Successfully built multitasking\n",
      "Installing collected packages: multitasking, yfinance\n",
      "Successfully installed multitasking-0.0.10 yfinance-0.1.70\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a96080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, precision_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gen_ret(df, tickers): # generates daily returns \n",
    "    for ticker in tickers:\n",
    "        ticker = ticker.lower()\n",
    "        df.loc[:,f\"ret_{ticker}\"] = df.loc[:,f\"adj close_{ticker}\"].pct_change()\n",
    "    return df\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n",
    "def gen_vol(df, tickers, lookback = 10): # generates ohlc volatility\n",
    "    import numpy as np\n",
    "    for ticker in tickers:\n",
    "        ticker = ticker.lower()\n",
    "        o = df.loc[:,'_'.join(['open', ticker])]\n",
    "        h = df.loc[:,'_'.join(['high', ticker])]\n",
    "        l = df.loc[:,'_'.join(['low', ticker])]\n",
    "        c = df.loc[:,'_'.join(['close', ticker])]\n",
    "\n",
    "        k = 0.34 / (1.34 + (lookback+1)/(lookback-1))\n",
    "        cc = np.log(c/c.shift(1))\n",
    "        ho = np.log(h/o)\n",
    "        lo = np.log(l/o)\n",
    "        co = np.log(c/o)\n",
    "        oc = np.log(o/c.shift(1))\n",
    "        oc_sq = oc**2\n",
    "        cc_sq = cc**2\n",
    "        rs = ho*(ho-co)+lo*(lo-co)\n",
    "        close_vol = cc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "        open_vol = oc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "        window_rs = rs.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "        result = (open_vol + k * close_vol + (1-k) * window_rs).apply(np.sqrt) * np.sqrt(252)\n",
    "        result[:lookback-1] = np.nan\n",
    "\n",
    "        df.loc[:,f'vol_{ticker}'] = result\n",
    "    return df\n",
    "\n",
    "def gen_rsi(df, tickers, periods = 14, ema = True): # technical indicator: Relative Strength Index\n",
    "    for ticker in tickers: \n",
    "        ticker = ticker.lower()\n",
    "        close_delta = df[f\"adj close_{ticker}\"].diff()\n",
    "\n",
    "        up = close_delta.clip(lower=0)\n",
    "        down = -1 * close_delta.clip(upper=0)\n",
    "\n",
    "        if ema == True:\n",
    "            ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "            ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        else:\n",
    "            ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "            ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "        rs = ma_up / ma_down\n",
    "        rsi = 100 - 100 / (1+rs)\n",
    "        df.loc[:,f\"rsi_{ticker}\"] = rsi\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gen_stok(df, tickers, periods = 14): # technical indicator: stocastic oscillator k%\n",
    "    \n",
    "    for ticker in tickers: \n",
    "        ticker = ticker.lower()\n",
    "        hh = df.loc[:,f\"high_{ticker}\"].rolling(periods).max()\n",
    "        ll = df.loc[:,f\"low_{ticker}\"].rolling(periods).min()\n",
    "        df.loc[:,f\"stok_{ticker}\"] = 100 * (df.loc[:,f\"adj close_{ticker}\"] - ll) / (hh - ll)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def gen_wilr(df, tickers, periods = 14): # technical indicator: William's R%\n",
    "    \n",
    "    for ticker in tickers: \n",
    "        ticker = ticker.lower()\n",
    "        hh = df.loc[:,f\"high_{ticker}\"].rolling(periods).max()\n",
    "        ll = df.loc[:,f\"low_{ticker}\"].rolling(periods).min()\n",
    "        df.loc[:,f\"wilr_{ticker}\"] = (hh - df.loc[:,f\"adj close_{ticker}\"]) / (hh - ll) * (-100)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gen_macd(df, tickers, periods = (12, 26, 9)): # technical indicator: MACD - MACD_signal\n",
    "    for ticker in tickers:\n",
    "        ticker = ticker.lower()\n",
    "        k = df[f\"adj close_{ticker}\"].ewm(span=periods[0], adjust=False, min_periods=periods[0]).mean()\n",
    "        d = df[f\"adj close_{ticker}\"].ewm(span=periods[1], adjust=False, min_periods=periods[1]).mean()\n",
    "        macd = k - d\n",
    "        macd_s = macd.ewm(span=periods[2], adjust=False, min_periods=periods[2]).mean()\n",
    "        df.loc[:,f\"macd_{ticker}\"] = macd - macd_s\n",
    "\n",
    "    return df\n",
    "\n",
    "def gen_obv(df, tickers, periods = 14): # technical indicator: OBV - OBV exponential MA \n",
    "    for ticker in tickers:\n",
    "        ticker = ticker.lower()\n",
    "        ac = data.loc[:,f\"adj close_{ticker}\"]\n",
    "        vo = data.loc[:,f\"volume_{ticker}\"]\n",
    "        obv = ((ac.diff() > 0)*1 - (ac.diff() < 0)*1) * vo\n",
    "        obv_ema = pd.DataFrame(obv).ewm(span = periods, adjust = False, min_periods = periods).mean()\n",
    "        df.loc[:,f\"obv_{ticker}\"] = obv_ema.values\n",
    "    return df\n",
    "\n",
    "def gen_retbbk(df, tickers, periods = 14): # technical indicator: Bollinger Band coefficient k\n",
    "    for ticker in tickers: \n",
    "        ticker = ticker.lower()\n",
    "        ret = data.loc[:, f\"ret_{ticker}\"] \n",
    "        ma = ret.rolling(periods).mean()\n",
    "        std = ret.rolling(periods).std()\n",
    "        df.loc[:,f\"retbbk_{ticker}\"] = (ret - ma)/std\n",
    "    return df\n",
    "        \n",
    "\n",
    "def gen_features(df, tickers):\n",
    "    df = gen_ret(df, tickers)\n",
    "    df = gen_vol(df, tickers)\n",
    "    df = gen_rsi(df, tickers)\n",
    "    df = gen_stok(df, tickers)\n",
    "    df = gen_wilr(df, tickers)\n",
    "    df = gen_macd(df, tickers)\n",
    "    df = gen_obv(df, tickers)\n",
    "    df = gen_retbbk(df, tickers)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def gen_lags(df, tickers, lags, lagged_cols, dropnan = True):\n",
    "    cols = list()\n",
    "    for i in range(lags, 0, -1):\n",
    "        lagged_df = df.loc[:,[x for x in lagged_cols if x in df.columns]].shift(i)\n",
    "        lagged_df.columns = ['_'.join([x, 'lag', str(i)]) for x in lagged_df.columns]\n",
    "        cols.append(lagged_df)\n",
    "    df = pd.concat([df]+cols, axis = 1)\n",
    "    if dropnan:\n",
    "        df = df.dropna(how = 'any')\n",
    "    return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79db6c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n",
      "         Date  adj close_aapl  adj close_amzn  adj close_fb  adj close_goog  \\\n",
      "0  2017-01-03       27.332468      753.669983    116.860001      786.140015   \n",
      "1  2017-01-04       27.301876      757.179993    118.690002      786.900024   \n",
      "2  2017-01-05       27.440720      780.450012    120.669998      794.020020   \n",
      "3  2017-01-06       27.746634      795.989990    123.410004      806.150024   \n",
      "4  2017-01-09       28.000782      796.919983    124.900002      806.650024   \n",
      "\n",
      "   adj close_nflx  adj close_spy  close_aapl  close_amzn    close_fb  ...  \\\n",
      "0      127.489998     206.147552   29.037500  753.669983  116.860001  ...   \n",
      "1      129.410004     207.373947   29.004999  757.179993  118.690002  ...   \n",
      "2      131.809998     207.209198   29.152500  780.450012  120.669998  ...   \n",
      "3      131.070007     207.950577   29.477501  795.989990  123.410004  ...   \n",
      "4      130.949997     207.264084   29.747499  796.919983  124.900002  ...   \n",
      "\n",
      "      open_fb   open_goog   open_nflx    open_spy  volume_aapl  volume_amzn  \\\n",
      "0  116.029999  778.809998  124.959999  225.039993    115127600      3521100   \n",
      "1  117.550003  788.359985  127.489998  225.619995     84472400      2510500   \n",
      "2  118.860001  786.080017  129.220001  226.270004     88774400      5830100   \n",
      "3  120.980003  795.260010  132.080002  226.529999    127007600      5986200   \n",
      "4  123.550003  806.400024  131.479996  226.910004    134247600      3446100   \n",
      "\n",
      "   volume_fb  volume_goog  volume_nflx  volume_spy  \n",
      "0   20663900      1657300      9437900    91366500  \n",
      "1   19630900      1073000      7843600    78744400  \n",
      "2   19492200      1335200     10185500    78379000  \n",
      "3   28545300      1640200     10657900    71559900  \n",
      "4   22880400      1274600      5771800    46939700  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "         Date  adj close_aapl  adj close_amzn  adj close_fb  adj close_goog  \\\n",
      "0  2017-01-03       27.332468      753.669983    116.860001      786.140015   \n",
      "1  2017-01-04       27.301876      757.179993    118.690002      786.900024   \n",
      "2  2017-01-05       27.440720      780.450012    120.669998      794.020020   \n",
      "3  2017-01-06       27.746634      795.989990    123.410004      806.150024   \n",
      "4  2017-01-09       28.000782      796.919983    124.900002      806.650024   \n",
      "\n",
      "   adj close_nflx  adj close_spy  close_aapl  close_amzn    close_fb  ...  \\\n",
      "0      127.489998     206.147552   29.037500  753.669983  116.860001  ...   \n",
      "1      129.410004     207.373947   29.004999  757.179993  118.690002  ...   \n",
      "2      131.809998     207.209198   29.152500  780.450012  120.669998  ...   \n",
      "3      131.070007     207.950577   29.477501  795.989990  123.410004  ...   \n",
      "4      130.949997     207.264084   29.747499  796.919983  124.900002  ...   \n",
      "\n",
      "   obv_aapl  obv_nflx  obv_goog  obv_spy  retbbk_fb  retbbk_amzn  retbbk_aapl  \\\n",
      "0       NaN       NaN       NaN      NaN        NaN          NaN          NaN   \n",
      "1       NaN       NaN       NaN      NaN        NaN          NaN          NaN   \n",
      "2       NaN       NaN       NaN      NaN        NaN          NaN          NaN   \n",
      "3       NaN       NaN       NaN      NaN        NaN          NaN          NaN   \n",
      "4       NaN       NaN       NaN      NaN        NaN          NaN          NaN   \n",
      "\n",
      "   retbbk_nflx  retbbk_goog  retbbk_spy  \n",
      "0          NaN          NaN         NaN  \n",
      "1          NaN          NaN         NaN  \n",
      "2          NaN          NaN         NaN  \n",
      "3          NaN          NaN         NaN  \n",
      "4          NaN          NaN         NaN  \n",
      "\n",
      "[5 rows x 85 columns]\n",
      "Date\n",
      "ret_fb\n",
      "ret_amzn\n",
      "ret_aapl\n",
      "ret_nflx\n",
      "ret_goog\n",
      "ret_spy\n",
      "vol_fb\n",
      "vol_amzn\n",
      "vol_aapl\n",
      "vol_nflx\n",
      "vol_goog\n",
      "vol_spy\n",
      "rsi_fb\n",
      "rsi_amzn\n",
      "rsi_aapl\n",
      "rsi_nflx\n",
      "rsi_goog\n",
      "rsi_spy\n",
      "stok_fb\n",
      "stok_amzn\n",
      "stok_aapl\n",
      "stok_nflx\n",
      "stok_goog\n",
      "stok_spy\n",
      "wilr_fb\n",
      "wilr_amzn\n",
      "wilr_aapl\n",
      "wilr_nflx\n",
      "wilr_goog\n",
      "wilr_spy\n",
      "macd_fb\n",
      "macd_amzn\n",
      "macd_aapl\n",
      "macd_nflx\n",
      "macd_goog\n",
      "macd_spy\n",
      "obv_fb\n",
      "obv_amzn\n",
      "obv_aapl\n",
      "obv_nflx\n",
      "obv_goog\n",
      "obv_spy\n",
      "retbbk_fb\n",
      "retbbk_amzn\n",
      "retbbk_aapl\n",
      "retbbk_nflx\n",
      "retbbk_goog\n",
      "retbbk_spy\n",
      "ret_fb_lag_3\n",
      "ret_amzn_lag_3\n",
      "ret_aapl_lag_3\n",
      "ret_nflx_lag_3\n",
      "ret_goog_lag_3\n",
      "ret_spy_lag_3\n",
      "vol_fb_lag_3\n",
      "vol_amzn_lag_3\n",
      "vol_aapl_lag_3\n",
      "vol_nflx_lag_3\n",
      "vol_goog_lag_3\n",
      "vol_spy_lag_3\n",
      "rsi_fb_lag_3\n",
      "rsi_amzn_lag_3\n",
      "rsi_aapl_lag_3\n",
      "rsi_nflx_lag_3\n",
      "rsi_goog_lag_3\n",
      "rsi_spy_lag_3\n",
      "stok_fb_lag_3\n",
      "stok_amzn_lag_3\n",
      "stok_aapl_lag_3\n",
      "stok_nflx_lag_3\n",
      "stok_goog_lag_3\n",
      "stok_spy_lag_3\n",
      "wilr_fb_lag_3\n",
      "wilr_amzn_lag_3\n",
      "wilr_aapl_lag_3\n",
      "wilr_nflx_lag_3\n",
      "wilr_goog_lag_3\n",
      "wilr_spy_lag_3\n",
      "macd_fb_lag_3\n",
      "macd_amzn_lag_3\n",
      "macd_aapl_lag_3\n",
      "macd_nflx_lag_3\n",
      "macd_goog_lag_3\n",
      "macd_spy_lag_3\n",
      "obv_fb_lag_3\n",
      "obv_amzn_lag_3\n",
      "obv_aapl_lag_3\n",
      "obv_nflx_lag_3\n",
      "obv_goog_lag_3\n",
      "obv_spy_lag_3\n",
      "retbbk_fb_lag_3\n",
      "retbbk_amzn_lag_3\n",
      "retbbk_aapl_lag_3\n",
      "retbbk_nflx_lag_3\n",
      "retbbk_goog_lag_3\n",
      "retbbk_spy_lag_3\n",
      "ret_fb_lag_2\n",
      "ret_amzn_lag_2\n",
      "ret_aapl_lag_2\n",
      "ret_nflx_lag_2\n",
      "ret_goog_lag_2\n",
      "ret_spy_lag_2\n",
      "vol_fb_lag_2\n",
      "vol_amzn_lag_2\n",
      "vol_aapl_lag_2\n",
      "vol_nflx_lag_2\n",
      "vol_goog_lag_2\n",
      "vol_spy_lag_2\n",
      "rsi_fb_lag_2\n",
      "rsi_amzn_lag_2\n",
      "rsi_aapl_lag_2\n",
      "rsi_nflx_lag_2\n",
      "rsi_goog_lag_2\n",
      "rsi_spy_lag_2\n",
      "stok_fb_lag_2\n",
      "stok_amzn_lag_2\n",
      "stok_aapl_lag_2\n",
      "stok_nflx_lag_2\n",
      "stok_goog_lag_2\n",
      "stok_spy_lag_2\n",
      "wilr_fb_lag_2\n",
      "wilr_amzn_lag_2\n",
      "wilr_aapl_lag_2\n",
      "wilr_nflx_lag_2\n",
      "wilr_goog_lag_2\n",
      "wilr_spy_lag_2\n",
      "macd_fb_lag_2\n",
      "macd_amzn_lag_2\n",
      "macd_aapl_lag_2\n",
      "macd_nflx_lag_2\n",
      "macd_goog_lag_2\n",
      "macd_spy_lag_2\n",
      "obv_fb_lag_2\n",
      "obv_amzn_lag_2\n",
      "obv_aapl_lag_2\n",
      "obv_nflx_lag_2\n",
      "obv_goog_lag_2\n",
      "obv_spy_lag_2\n",
      "retbbk_fb_lag_2\n",
      "retbbk_amzn_lag_2\n",
      "retbbk_aapl_lag_2\n",
      "retbbk_nflx_lag_2\n",
      "retbbk_goog_lag_2\n",
      "retbbk_spy_lag_2\n",
      "ret_fb_lag_1\n",
      "ret_amzn_lag_1\n",
      "ret_aapl_lag_1\n",
      "ret_nflx_lag_1\n",
      "ret_goog_lag_1\n",
      "ret_spy_lag_1\n",
      "vol_fb_lag_1\n",
      "vol_amzn_lag_1\n",
      "vol_aapl_lag_1\n",
      "vol_nflx_lag_1\n",
      "vol_goog_lag_1\n",
      "vol_spy_lag_1\n",
      "rsi_fb_lag_1\n",
      "rsi_amzn_lag_1\n",
      "rsi_aapl_lag_1\n",
      "rsi_nflx_lag_1\n",
      "rsi_goog_lag_1\n",
      "rsi_spy_lag_1\n",
      "stok_fb_lag_1\n",
      "stok_amzn_lag_1\n",
      "stok_aapl_lag_1\n",
      "stok_nflx_lag_1\n",
      "stok_goog_lag_1\n",
      "stok_spy_lag_1\n",
      "wilr_fb_lag_1\n",
      "wilr_amzn_lag_1\n",
      "wilr_aapl_lag_1\n",
      "wilr_nflx_lag_1\n",
      "wilr_goog_lag_1\n",
      "wilr_spy_lag_1\n",
      "macd_fb_lag_1\n",
      "macd_amzn_lag_1\n",
      "macd_aapl_lag_1\n",
      "macd_nflx_lag_1\n",
      "macd_goog_lag_1\n",
      "macd_spy_lag_1\n",
      "obv_fb_lag_1\n",
      "obv_amzn_lag_1\n",
      "obv_aapl_lag_1\n",
      "obv_nflx_lag_1\n",
      "obv_goog_lag_1\n",
      "obv_spy_lag_1\n",
      "retbbk_fb_lag_1\n",
      "retbbk_amzn_lag_1\n",
      "retbbk_aapl_lag_1\n",
      "retbbk_nflx_lag_1\n",
      "retbbk_goog_lag_1\n",
      "retbbk_spy_lag_1\n"
     ]
    }
   ],
   "source": [
    "tickers = [\"FB\", \"AMZN\", \"AAPL\", # FAANG stocks \n",
    "           \"NFLX\", \"GOOG\", \n",
    "           \"SPY\",]\n",
    "# tickers = ['SPY',\"AAPL\"]\n",
    "start = \"2017-1-1\"\n",
    "end = \"2022-1-31\"\n",
    "data = yf.download(tickers, start, end, interval = \"1d\")\n",
    "data.columns = ['_'.join(col).lower().strip() for col in data.columns.values]\n",
    "data.to_csv(\"data.csv\", index = True)\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "print(data.head(5))\n",
    "data = gen_features(data, tickers = tickers)\n",
    "print(data.head(5))\n",
    "lagged_cols = [['_'.join([str(x).lower(), str(y).lower()]) for y in tickers] \n",
    "                               for x in [\n",
    "                                        'ret', \n",
    "                                         'vol', \n",
    "                                        'rsi', \n",
    "                                         'stok', \n",
    "                                         'wilr',\n",
    "                                        'macd', \n",
    "                                         'obv', \n",
    "                                         'retbbk'\n",
    "                                        ]\n",
    "                              ]\n",
    "data = gen_lags(data, tickers = tickers, lags = 3, \n",
    "                lagged_cols = [item for sublist in lagged_cols for item in sublist]\n",
    "               )\n",
    "data = data.loc[:,[x for x in data.columns if \n",
    "                   \"high\" not in x \n",
    "                   and \"low\" not in x\n",
    "                   and \"close\" not in x\n",
    "                   and \"open\" not in x\n",
    "                   and \"volume\" not in x]\n",
    "               ]\n",
    "for col in data.columns:\n",
    "    print(col, end = \"\\n\")\n",
    "data = data.dropna(how = 'any', axis = 0)\n",
    "data.to_csv(\"featured_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8662910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_obj(df, target_col):\n",
    "    obj = {\"start\": df.loc[:,'Date'].tolist()[0], \"target\": df.loc[:,target_col].tolist()}\n",
    "    for key, val in df.loc[:,\n",
    "                          [x for x in df.columns if (x != target_col and x.lower() != 'date')]\n",
    "                          ].to_dict().items():\n",
    "        obj[key] = list(val.values())\n",
    "    \n",
    "    return json.dumps(obj)\n",
    "\n",
    "def split_data(df, train_size, test_size, target_col):\n",
    "    encoding = \"utf-8\"\n",
    "    output_dir = os.path.join(os.getcwd(), \"data\")\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    FILE_TRAIN = os.path.join(\"data\", \"train.json\")\n",
    "    FILE_VALID = os.path.join(\"data\", \"validation.json\")\n",
    "    FILE_TEST = os.path.join(\"data\", \"test.json\")\n",
    "    \n",
    "    # seperate post 2020 data out for backtesting\n",
    "    \n",
    "    filt_post20 = df.loc[:,'Date'].apply(\n",
    "        lambda x : int(str(x).split('-')[0])>2020\n",
    "    )\n",
    "#     print(df.loc[filt_post20,:])\n",
    "    with open(FILE_TEST, \"wb\") as f0:\n",
    "        f0.write(frame_to_obj(df.loc[filt_post20,:], target_col).encode(encoding))\n",
    "        f0.write(\"\\n\".encode(encoding))\n",
    "        \n",
    "    df = df.loc[~filt_post20]\n",
    "    tsplit = TimeSeriesSplit(max_train_size=train_size, \n",
    "                             test_size = test_size, \n",
    "                             n_splits = len(df)-train_size-test_size+1\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    with open(FILE_TRAIN, \"wb\") as f1:\n",
    "        with open(FILE_VALID, \"wb\") as f2:\n",
    "            for train_index, test_index in tsplit.split(df):\n",
    "                train_idx = train_index\n",
    "                test_idx = list(train_index)+list(test_index)\n",
    "                f1.write(frame_to_obj(df.iloc[train_idx], target_col).encode(encoding))\n",
    "                f1.write(\"\\n\".encode(encoding))\n",
    "                f2.write(frame_to_obj(df.iloc[test_idx], target_col).encode(encoding))\n",
    "                f2.write(\"\\n\".encode(encoding))\n",
    "    return \"Files written. \\\"test.json\\\" and \\\"train.json\\\" created/overwritten.\"\n",
    "\n",
    "def split_data_csv(df, train_size, test_size):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    output_dir = os.path.join(os.getcwd(), \"data\")\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    FILE_TRAIN = os.path.join(\"data\", \"train\")\n",
    "    FILE_VALID = os.path.join(\"data\", \"validation\")\n",
    "    FILE_TEST = os.path.join(\"data\", \"test\")\n",
    "    filt_post20 = df.loc[:,'Date'].apply(\n",
    "        lambda x : int(str(x).split('-')[0])>2020\n",
    "    )\n",
    "#     print(df.loc[filt_post20,:])\n",
    "    df.loc[filt_post20,\n",
    "           [x for x in df.columns if \"_\" in x  or x.lower() == \"target\"]\n",
    "          ].to_csv(FILE_TEST+'.csv', header = False, index = False) # This is for S3 channel\n",
    "    df.loc[filt_post20, \n",
    "           [x for x in df.columns if \"_\" in x  or x.lower() == \"target\"\n",
    "           ]\n",
    "          ].to_csv(FILE_TEST+'_header.csv', header = True, index = False) # This is for trading simulation\n",
    "    df = df.loc[~filt_post20, \n",
    "                [x for x in df.columns if x.lower() != 'date']\n",
    "               ]\n",
    "    print(df.columns)\n",
    "    \n",
    "    df.to_csv(FILE_TRAIN+'.csv', header = False, index = False)\n",
    "\n",
    "    return \".csv files written.\"\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4cd3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['target', 'ret_fb', 'ret_amzn', 'ret_aapl', 'ret_nflx', 'ret_goog',\n",
      "       'ret_spy', 'vol_fb', 'vol_amzn', 'vol_aapl',\n",
      "       ...\n",
      "       'obv_aapl_lag_1', 'obv_nflx_lag_1', 'obv_goog_lag_1', 'obv_spy_lag_1',\n",
      "       'retbbk_fb_lag_1', 'retbbk_amzn_lag_1', 'retbbk_aapl_lag_1',\n",
      "       'retbbk_nflx_lag_1', 'retbbk_goog_lag_1', 'retbbk_spy_lag_1'],\n",
      "      dtype='object', length=193)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.csv files written.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ticker = \"spy\" # we predict\n",
    "# frame_to_obj(data.iloc[:20,:], \"ret_msft\")\n",
    "data = pd.read_csv(\"featured_data.csv\").loc[:,\n",
    "#                                             [x for x in data.columns if \n",
    "#                                              'spy' in x.lower() \n",
    "#                                              or \n",
    "#                                              'date' in x.lower()\n",
    "#                                             ]\n",
    "                                           ]\n",
    "\n",
    "data.loc[:,'target'] = ((data.loc[:,f\"ret_{target_ticker}\"].shift(-1) < 0.005) * 1)\n",
    "\n",
    "# cols = data.columns.tolist()\n",
    "# cols = cols[-1:] + cols[:-1]\n",
    "# data = data[cols]\n",
    "\n",
    "# Put target to front first. \n",
    "# split_data(data, \n",
    "#            train_size = 40, \n",
    "#            test_size = 1, \n",
    "#            target_col = \"target\")\n",
    "split_data_csv(data, \n",
    "           train_size = -1, \n",
    "           test_size = -1, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8b81c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "prefix = \"sagemaker/binaryforecaster\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3_data_path = f\"{bucket}/{prefix}/data\"\n",
    "s3_output_path = f\"{bucket}/{prefix}/output\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "# FILE_TRAIN = os.path.join(\"data\", \"train.json\")\n",
    "# FILE_VALID = os.path.join(\"data\", \"validation.json\")\n",
    "# FILE_TEST = os.path.join(\"data\", \"test.json\")\n",
    "for f in os.listdir(\"data\"):\n",
    "    if \".csv\" in f:\n",
    "        if \"test\" in f:\n",
    "            s3.upload_file(os.path.join(\"data\",f), bucket, prefix + \"/data/test/\" + f)\n",
    "        elif \"train\" in f:\n",
    "            s3.upload_file(os.path.join(\"data\",f), bucket, prefix + \"/data/train/\" + f)\n",
    "        \n",
    "# s3.upload_file(FILE_TRAIN, bucket, prefix + \"/data/train/\" + FILE_TRAIN)\n",
    "# s3.upload_file(FILE_VALID, bucket, prefix + \"/data/validation/\" + FILE_VALID)\n",
    "# s3.upload_file(FILE_TEST, bucket, prefix + \"/data/test/\" + FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04c654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41efe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
